



## 10.梯度的计算 ##




求线性直线的导函数，导函数表示的就是直线的**梯度`(gradient)`**。

多变量函数的情况下，梯度是导函数的一般化，求得的梯度实际上并不是导函数的标量而是向量函数。

机械学习的主要目的，是寻找和数据相一致的最优模型。这里的最优指的是损失函数(目的函数)最小。

梯度是用来寻找损失函数，或者叫费用函数最小的系数或函数值的方法。

我们已知找寻最适点地方法，是将目的函数的导函数求出并令其等于零求解，从而找到模型系数的方法。

系数两个以上的情况下，得到的并不是导函数，而是梯度；得到的也并不是标量值而是向量方程。

通过梯度可以找到“对于所有的点都指向函数的下一个极小值的向量”。

这是寻找函数最小值的最适化手法中非常常用的方法，通过计算各点的梯度，再反复将其系数向下一个极小值方向移动，从而找到最小值。

# Numpy中，可以使用`gradient()`函数来计算梯度。#
1)
```python
a = np.array([1, 3, 6, 7, 11, 14])
gr = np.gradient(a)
a
```
**array([2. , 2.5, 2. , 2.5, 3.5, 3. ])**
    
这个梯度的具体计算方法如下。
      
**gr[0] = (a[1] - a[0]) / 1 = 2  
gr[1] = (a[2] - a[0]) / 2 = 2.5  
gr[2] = (a[3] - a[1]) / 2 = 2  
gr[3] = (a[4] - a[2]) / 2 = 2.5  
gr[4] = (a[5] - a[3]) / 2 = 3.5  
gr[5] = (a[5] - a[4]) / 1 = 3**
      
# 下面看一下二次元数组的梯度计算。#

2)
```python
a = np.array([1, 3, 6, 7, 11, 14]).reshape(2, 3)
gr = np.gradient(a)
a
```
**[array([[6., 8., 8.],
        [6., 8., 8.]]), array([[2. , 2.5, 3. ],
        [4. , 3.5, 3. ]])]**
                            
# 二次元数组求出的梯度分别是各列和各行的计算。#

因此返回的结果是两个数组，第一个数组是行方向，后一个是表示列方向。
  
    
    
    
    
