


# 22.损失函数和误差函数

前面讲解了监督式学习和无监督学习。

但是不管哪种机械学习算法，最重要的可以就是最适化问题。

最适化函数中实际执行的是，使损失函数最小化。

让我们想象一下想要将每个月的储蓄额给最适化的情况。

在封闭状态下，实际上就是让每个月的消费，也就是损失函数最小化。

构筑损失函数，一般都是从预测值和实际值的差开始的。

一般情况下，先对已制成模型的参数进行推理，然后进行预测。

对预测进行评价所要求出的主要指标就是计算与实际值之间的差。

损失函数根据模型的不同是不同的。

比如说，平均二乘误差在回归模型中是适用的，但是在分类模型的损失函数中却并不适合。

下面简单的介绍一些损失函数及其特点。

(来源节取自知乎,https://zhuanlan.zhihu.com/p/58883095)

损失函数用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

损失函数分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

## 常见的损失函数以及其优缺点如下：

### 0-1损失函数(zero-one loss)
0-1损失是指预测值和目标值不相等为1， 否则为0:

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossz.svg)

特点：

(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2)**感知机**就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossz2.svg) 时认为相等，

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossz1.svg)

### 2. 绝对值损失函数

绝对值损失函数是计算预测值与目标值的差的绝对值：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/l1.svg)

### 3. log对数损失函数

log对数损失函数的标准形式如下：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/losslog.svg)

特点：

(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。

(2)健壮性不强，相比于hinge loss对噪声更敏感。

(3)**逻辑回归**的损失函数就是log对数损失函数。

### 4. 平方损失函数

平方损失函数标准形式如下：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/l2.svg)

特点：

(1)经常应用与回归问题

### 5. 指数损失函数（exponential loss）

指数损失函数的标准形式如下：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossexp.svg)

特点：

(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。

### 6. Hinge 损失函数

Hinge损失函数标准形式如下：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossh.svg)

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossh3.svg) 。**SVM**就是使用这个损失函数。

(2)一般的 ![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossh1.svg) 是预测值，在-1到1之间， *y* 是目标值(-1或1)。其含义是， ![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossh1.svg)的值在-1和+1之间就可以了，并不鼓励 ![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossh2.svg) ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而**使分类器可以更专注于整体的误差**。

(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。

### 7. 感知损失(perceptron loss)函数

感知损失函数的标准形式如下：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/lossper.svg)

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss**只要样本的判定类别正确的话，它就满意，不管其判定边界的距离**。它比Hinge loss简单，因为不是max-margin boundary，所以**模型的泛化能力没 hinge loss强**。

### 8. 交叉熵损失函数 (Cross-entropy loss function)

交叉熵损失函数的标准形式如下:

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/losscross.svg)

注意公式中 *x* 表示样本， *y* 表示预测的输出， *a* 表示实际的输出， *n* 表示样本总数量。

特点：

(1)本质上也是一种**对数似然函数**，可用于二分类和多分类任务中。

二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/losscross1.svg)

多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/losscross2.svg)

(2)当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢的问题**，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

机械学习的算法中，损失函数的变量反复更新这件事是不可或缺的。

反复计算误差并持续将最小误差返回更新，就是对于模型的训练。

以为损失函数可以直接影响到模型的参数，因此正确的损失函数的使用也会直接影响到机械学习算法的精度。

