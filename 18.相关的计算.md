



# 18.相关的计算

将双变量解析特化后，可以进行双列的解析。

一般称有着这样关系的两个函数**相关**。

相关表示两个变量之间的关系，比如当变量B增加了10%，变量A如何变化之类的问题。

这里依旧使用波士顿市的住宅价格数据集来说明相关的计算方法以及所得到的二次元分布图。

一般来说所，相关在统计中表示具有依存性。

相关系数就是指计算相关指标的定量。

相关和相关系数的关系类似于湿度计与湿度的关系。

最经常使用的相关系数之一，叫做皮尔逊积矩相关系数(又称作 PPMCC或PCCs)。

相关系数的值的范围是从+1到-1，这里的-1是两个变量中更强的负线性关系，而+1是更强的正线性关系。

Numpy中可以使用`corrcoef()`来计算相关行列。

```python
np.set_printoptions(suppress=True, linewidth=125)
CorrelationCoef_Matrix = np.round(np.corrcoef(samples, rowvar=False), decimals=1)
CorrelationCoef_Matrix
```
    array([[ 1. , -0.2,  0.4, -0.1,  0.4, -0.2,  0.4, -0.4,  0.6,  0.6,  0.3, -0.4,  0.5],
           [-0.2,  1. , -0.5, -0. , -0.5,  0.3, -0.6,  0.7, -0.3, -0.3, -0.4,  0.2, -0.4],
           [ 0.4, -0.5,  1. ,  0.1,  0.8, -0.4,  0.6, -0.7,  0.6,  0.7,  0.4, -0.4,  0.6],
           [-0.1, -0. ,  0.1,  1. ,  0.1,  0.1,  0.1, -0.1, -0. , -0. , -0.1,  0. , -0.1],
           [ 0.4, -0.5,  0.8,  0.1,  1. , -0.3,  0.7, -0.8,  0.6,  0.7,  0.2, -0.4,  0.6],
           [-0.2,  0.3, -0.4,  0.1, -0.3,  1. , -0.2,  0.2, -0.2, -0.3, -0.4,  0.1, -0.6],
           [ 0.4, -0.6,  0.6,  0.1,  0.7, -0.2,  1. , -0.7,  0.5,  0.5,  0.3, -0.3,  0.6],
           [-0.4,  0.7, -0.7, -0.1, -0.8,  0.2, -0.7,  1. , -0.5, -0.5, -0.2,  0.3, -0.5],
           [ 0.6, -0.3,  0.6, -0. ,  0.6, -0.2,  0.5, -0.5,  1. ,  0.9,  0.5, -0.4,  0.5],  
           [ 0.6, -0.3,  0.7, -0. ,  0.7, -0.3,  0.5, -0.5,  0.9,  1. ,  0.5, -0.4,  0.5],
           [ 0.3, -0.4,  0.4, -0.1,  0.2, -0.4,  0.3, -0.2,  0.5,  0.5,  1. , -0.2,  0.4],
           [-0.4,  0.2, -0.4,  0. , -0.4,  0.1, -0.3,  0.3, -0.4, -0.4, -0.2,  1. , -0.4],
           [ 0.5, -0.4,  0.6, -0.1,  0.6, -0.6,  0.6, -0.5,  0.5,  0.5,  0.4, -0.4,  1. ]]) 

*seaborn*是基于*matplotlib*而发展出的统计数据可视化库，可以制作出非常漂亮且具有魅力的数据统计图。

作为一款十分有人气的库，可以与包括*pandas*在内的许多包进行完美的互换。

seaborn包中可以使用密度图将相关行列可视化。

当具有数百个特征的情况下，对找出高相关性的系数具有很大的帮助。

```python
CorrelationCoef_Matrix1 = np.round(np.corrcoef(samples, rowvar=False), decimals=1)
CorrelationCoef_Matrix1
%matplotlib inline # “%matplot inline” 是为了在 Jupyter 中正常显示图形，若没有这行代码，图形显示不出来的。
import seaborn as sns; sns.set()
ax = sns.heatmap(CorrelationCoef_Matrix1, cmap="YlGnBu")
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/heatmap1.png)

下面求一下特征与标签列的相关系数。

就像下例中追加标签列一样，`corrcoef()`函数的第二引数可以用来追加数列的组。

这里一定要形状相同，`corrcoeaf()`函数是直接将该列合并到最后，再进行计算相关行列。

```python
np.set_printoptions(suppress=True, linewidth=125)
CorrelationCoef_Matrix2 = np.round(np.corrcoef(samples, label, rowvar=False), decimals=2)
print("    F1    F2    F3    F4    F5    F6    F7    F8    F9    F10   F11   F12   F13")
print(CorrelationCoef_Matrix2[0:13, 13:14].T)
```
         F1    F2    F3    F4    F5    F6    F7    F8    F9    F10   F11   F12   F13
    [[-0.39  0.36 -0.48  0.18 -0.43  0.7  -0.38  0.25 -0.38 -0.47 -0.51  0.33 -0.74]]

可以看到，除去F13以外，大部分的特征都具有较弱或中程度负线性关系。

相反，F6具有较强的正线性关系。

这里将特征转换为分布图，进一步观察其相关性。

下面将借助matplotlib，图示特征('RM','DIS','LSTAT‘)与标签列组成的三个不同的分布图。

```python
%matplotlib notebook
import matplotlib.pyplot as plt
from scipy import stats
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))
axs = [ax1, ax2, ax3]
feature_list = [samples[:, 5:6], samples[:, 7:8], samples[:, 12:13]]
feature_names = ["RM", "DIS", "LSTAT"]
for n in range(0, len(feature_list)):
    axs[n].scatter(label, feature_list[n], edgecolors=(0, 0, 0))
    axs[n].set_ylabel(feature_names[n])
    axs[n].set_xlabel('label')
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/heatmap2.png)

上面的代码中，RM和标签的值是正的线性直线状分布，和前面所求出的0.7相关系数相吻合。

这个分布图中，RM的值越高，标签的值就越高。

中间的分布图，是相关系数为2.5的情况。

数据的分布比较散乱，没有明显的关系。

右边的分布图是表示的是相关系数为-0.7的强负线性关系。

随着LSTAT的值减小，标签的值会逐渐增大。

上面的相关行列和分布图全部都是在没有进行过修剪的基础下计算的得出的。

下面我们将各特征和标签的数据从两侧修剪10%，再观察得出的线性关系有着怎样的变化。

```python
%matplotlib notebook
import matplotlib.pyplot as plt
from scipy import stats
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 4))
axs = [ax1, ax2, ax3]
RM_tr = stats.trimboth(samples[:, 5:6], 0.1)
label_tr = stats.trimboth(label, 0.1)
LSTAT_tr = stats.trimboth(samples[:, 12:13], 0.1)
DIS_tr = stats.trimboth(samples[:, 7:8], 0.1)
feature_names = ["RM", "DIS", "LSTAT"]
feature_list = [RM_tr, DIS_tr, LSTAT_tr]
for n in range(0, len(feature_list)):
    axs[n].scatter(label_tr, feature_list[n], edgecolors=(0, 0, 0))
    axs[n].set_ylabel(feature_names[n])
    axs[n].set_xlabel('label')
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/heatmap3.png)

对数据进行修剪后，可以发现三个特征和标签所对应的都是强正线性相关。

特别是，DIS和LSTAT数据中有着十分明显的变化。

修剪的威力在这里一目了然。

若没有对离群值进行操作，哪怕很简单的数据也很容易造成较大的误解或误差。

离群值可能会影响分布的形状和其与其他变量的相关性，最终导致数据模型的性能收到影响。

目前我们了解了许多方法对数据进行必要的操作和探索性的分析。

这些都可以帮助我们了解和使用数据。

**接下来我们将学习通过线性回归对数据进行预测**。
