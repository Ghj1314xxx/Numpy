    
    
    
    
# 23.使用梯度下降法的线性单回归

本节将使用前面介绍探索性数据分析所使用的波士顿市住宅价格数据集来对线性单回归进行实装。

在找寻契合的回归直线前，我们需要先对必要的库进行声明并将数据集读取出来。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.datasets import load_boston
dataset = load_boston()
samples, label, feature_names = dataset.data, dataset.target, dataset.feature_names

bostondf = pd.DataFrame(dataset.data)
bostondf.columns = dataset.feature_names
bostondf['Target Price'] = dataset.target
bostondf.head()
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/table1.png)

之前我们使用过Numpy数组，但是这里我们学习一个更便利的数据构造**数据帧**。

我们将使用pandas库内的数据帧。

假如数据只有数值，大部分情况下，数据存储到Numpy数组还是pandas的数据帧在技术上是没有区别的。

所以我们将目标值追加到数据帧，然会通过散步图观察一下特征RM和目标值之间的关系。

```python
imort matplotlib.pyplot as plt
bostondf.plot(x='RM', y='Target Price', styel='o')
plt.title('RM vs Target Price')
plt.show()
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/tar.png)

从图中就可以看出，每栋楼的平局房间数和住宅价格和预想的相同成正相关。

接下来，我们调查一下这种关系的程度，然后通过使用这种关系来进行住宅价格的预测。

假设，我们关于线性回归的知识非常非常有限。

虽然式子都很熟悉，但是误差函数是什么？为什么反复是必要的？梯度下降是什么？
为什么回归模型使用梯度下降法？这些问题都完全不知道。

那么这种情况下，你会怎么做呢？

计算预测值开始之前，我们先了解一下式子的系数和切片（截矩）的初期值如何给出。

首先计算出多个预测值，然后与实际的值进行比较，确认他们与现实有多大的差距。

然后，尝试将系数或者截矩或者双方的值进行改变，确认结果是否向实际的值靠近。

有没有感觉这个过程就是很自然，顺理成章地事情。

事实上，线性回归模型的算法，更娴熟的进行同样的处理。

为了更好的理解线性回归模型的执行顺序，将代码分块进行说明。

**最初，首先制作想线性回归结果作为预测值返回的函数**。

```python
def prediction(X, coefficient, intercept):
    return X * coefficient + intercept
```

上面的函数执行的就是下面的这个线性回归模型的计算。

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/line.svg)

**接下来损失函数是必须的**。

这里需要进行反复的计算。

`cost_function`是使用平均二乘误差。

这里就是预测值和实际的值之间的差的二次方的和的平均。

```python
def cost_function(X, Y, coefficient, intercept):
    MSE = 0.0
    for i in range(len(X)):
        MSE += (Y[i] - (coefficient * X[i] + intercept)) ** 2
    return MSE / len(X)
```

**最后一部分代码使用进行`重要度（权重）`的更新**。

这里的重要度，不仅仅是独立变量的系数的，也表示截矩的重要度。

为了对重要度进行逻辑上的更新，寻找给予函数的最小值，所以反复最适化算法是必要的。

比如说，反复使用梯度下降法使得损失函数最小化。

下面让我们一步一步追踪梯度下降法的顺序。

首先，对截矩和系数的重要度进行初期化，对平均二乘误差进行计算也是必要的。

接下来，观察重要度的变化对平均二乘误差会造成怎样的影响，从而计算斜率。

为了更好的修正重要度，掌握系数和截矩如何变化更好是十分必要的。

总而言之，修正重要度时，一定要计算出误差函数的斜率。

斜率可以通过对系数和截矩的损失函数进行偏微分求出。

线性单回归的系数只有一个，通过偏微分计算后，算法将重要度调整后再次计算平均二乘误差。

这个过程，反复进行直到改变重要度也无法让平均二乘误差变得更小为止。

```python
def update_weights(X, Y, coefficient, intercept, learning_rate)
    coefficient_derivative = 0
    intercept_derivative = 0
    for i in range(len(X)):
        coefficient_derivative += - 2 * X[i] * (Y[i] - (coefficient * Xx[i] + intercept))
        intercept_derivative += - 2 * (Y[i] - (coefficient * X[i] + intercept))
    coefficient -= (coefficient_derivative / len(X)) * learning_rate
    intercept -= (intercept_derivative / len(X)) * learning_rate
    return coefficient, intercept
```

上面的代码部分，是定义了重要度的更新，且更新后返回系数和截矩的函数。

这个函数还有一个十分重要的参数就是`learning_rate（学习率）`。

**学习率决定了变化的大小**。

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/grad.svg)

上图可以看出，损失函数的二次方和是一个二次方图像形式。

梯度下降法就像图像所示那样，为了寻找损失的最小值，通过偏微分不停的迫近最接近0的点。

如前面所述，算法是从重要度的初始化开始的，也就意味着是从远离最小值的点开始进行的。

算法通过反复进行重要度更新，然后让损失值越来越小。

这也就意味着在进行了足够多的反复后，一定会收束到最小值。

**而学习率，决定了收束的速度**。

也就是说，学习率高的情况下，每次重要度更新后，从这个点到下个点都有一个比较大的跳跃。

而学习率较低的情况下，向最小值迫近的速度就会很慢。

因为学习率是超参数，因此在实行前一定要提前设定好。

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/big.svg)
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/small.svg)

那么，学习率的值究竟是大一点还是小一点呢。

设定的学习率大的情况下，很容易直接跳过最小值。

若跨越过大，甚至可能无法向最小值收束，从而导致无法得出最小值。

相反学习率小的情况下，则到收束为止需要进行反复的次数足够多。

前面作为部件的代码已经准备好了，下面开始写主函数。

主函数需要将前面的流程按照顺序走一遍。

![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/liucheng.PNG)

下面是主函数部分代码。

```python
def train(X, Y, coefficient, intercept, LearningRate, iteration):
    cost_hist = []
    for i in range(iteration):
        coefficient, intercept = update_weights(X, Y, coefficient, intercept, learning_rate)
        cost = cost_function(X, Y, coefficient, intercept)
        cost_hist.append(cost)
    return coefficient, intercept, cost_hist
```

这样我们所有的代码块都已经定义完成了，因此可以准备实行单变量模型。

`train()`函数实行前，需要对其的超参数、系数和截矩进行初期化设定。

下面，我们生成变量、设定初始值以及将值所谓函数的参数。

```python
learning_rate = 0.01
iteration = 10001
coefficient = 0.3
intercept = 2
X = bostondf.iloc[:, 5:6].values
Y = bostondf.iloc[:, 13:14].values

coefficient, intercept, cost_history = train(X, Y, coefficient, intercept, learning_rate, iteration)
```

当然我们呼出函数时，将这些值作为关键字参数进行定义也是同样可以的。

```python
#coefficient, intercept, cost_history = train(X, Y, coefficient, intercept=2, learning_rate=0.01, iteration=10001)
```

上面定义了主函数后，将截矩与系数的最终值分别存储于两个数组中返回。

接下来，主函数将返回每次反复得到的损失值得结果，也就是平均二乘误差的列表。

这个列表对于追踪每次反复后损失的变化非常有帮助。

```python
coefficient
```
    --->array([8.57526661])

```python
intercept
```
    --->array([-31.31931428])

```python
cost_history
```
    --->[array([72.59720667]),
         array([60.96727513]),
         array([60.58066489]),
         array([60.56018189]), 
         array([60.55162513]),
         array([60.54346052]),
         array([60.53531248]),
         array([60.52716876]),
         ...]
     
从系数的值可以知道，RM每增加一单位，住宅价格大约增长$8,757。

那么，将通过回归方程求出的截矩与系数代入，就可以求出预测值。

接下来，我们将线性回归曲线图示化，观察一下与数据是否契合。

```python
y_hat = X * coefficient + intercept
plt.plot(X, Y, 'bo')
plt.plot(X, y_hat)
plt.show()
```
![](https://github.com/Ghj1314xxx/Numpy/blob/master/Images/y_hat.png)

本节我们选择了一个变量，进行单变量模型的适用。

后面我们将向模型追加独立函数，从而实行多变量线性回归模型。

也就是说，为了得到最适的结果，使得最适化的系数为复数的情况。





